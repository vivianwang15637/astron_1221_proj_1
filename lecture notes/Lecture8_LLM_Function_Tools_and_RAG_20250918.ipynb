{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models: Function Tools and Retrieval Augmented Generation\n",
    "\n",
    "*Tutorial by Yuan-Sen Ting*\n",
    "\n",
    "*Astron 1221: Lecture 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: From Text to Tools\n",
    "\n",
    "### Teaching Your AI Assistant to Calculate\n",
    "\n",
    "Last lecture, you mastered the art of programmatic conversation with Claude. You learned to extract structured data from observation logs, analyze astronomical images, and build conversation systems that maintain context. But if you're like most students after Lecture 7, you probably noticed something: Claude was helpful at understanding and explaining astronomical concepts, but when it came to actual calculations, you were still doing all the mathematical work yourself.\n",
    "\n",
    "Today, that changes. You're about to cross the threshold from having an AI that talks about astronomy to having an AI that does astronomy. By the end of this lecture, you'll command an AI assistant that can calculate stellar parallaxes, determine orbital periods, analyze light curves, and search through your entire course knowledge base—all while you focus on the science rather than the implementation details.\n",
    "\n",
    "Consider this scenario: You're analyzing a dataset of binary star observations for your research project. You have radial velocity measurements over time, and you need to determine the orbital period and calculate the system's total mass. In the pre-function-tools world, this meant hours of looking up formulas, writing NumPy code, debugging array operations, and manually searching through lecture notes for the relevant theory.\n",
    "\n",
    "In the post-function-tools world, you simply say: \"Analyze this binary star dataset—determine the orbital period and calculate the total mass.\" Claude automatically calls your period-finding function with the radial velocity data, executes mass calculations using Kepler's laws, and returns a complete analysis with both computational results and theoretical context.\n",
    "\n",
    "This isn't about replacing your astronomical knowledge—it's about amplifying it. Every function Claude calls uses physics you understand. Every calculation builds on mathematical concepts you've learned. But now these capabilities operate at machine speed and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Function Tools Transform Your Research\n",
    "\n",
    "The transformation we're making today fundamentally shifts how you interact with computation. Instead of an assistant that knows things, you get an assistant that can do things.\n",
    "\n",
    "**Claude as Information Source (Lecture 7):**\n",
    "- You: \"What's the formula for stellar luminosity?\"\n",
    "- Claude: \"Here's the Stefan-Boltzmann law: L = 4πR²σT⁴\"\n",
    "- You: Spend 20 minutes implementing this in NumPy, debugging array shapes\n",
    "\n",
    "**Claude as Computational Partner (Today):**\n",
    "- You: \"Calculate the luminosity of this star given its radius and temperature\"\n",
    "- Claude: Directly calls your `stellar_luminosity()` function with the parameters\n",
    "- Claude: Returns the calculated result immediately, plus physical interpretation\n",
    "\n",
    "But here's what matters: you're not becoming less capable—you're becoming more powerful. Every function Claude calls is one you understand and could write yourself (using skills from Lectures 1-6). Every calculation follows physics principles you've learned. Claude handles the mechanical execution; you provide the scientific direction, validation, and creative thinking.\n",
    "\n",
    "This workflow mirrors how professional astronomy works. Research astronomers don't rewrite basic calculations from scratch for every project. They build libraries of tested functions and focus their mental energy on novel scientific questions. Today, you start building those libraries and learning to orchestrate them through AI collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Function Tools\n",
    "\n",
    "### What Are Function Tools?\n",
    "\n",
    "Function tools represent a fundamental shift in how LLMs interact with your code. Until now, when you asked Claude to calculate something, it would describe the calculation process in text. You then had to implement that calculation yourself in Python. Function tools eliminate this middle step—Claude can now directly execute Python functions you've written.\n",
    "\n",
    "Think of it like the difference between having an assistant who can only read instruction manuals versus one who can actually operate the equipment. The first can tell you how to use a telescope; the second can actually point it at the stars and take measurements.\n",
    "\n",
    "Here's the key concept: you write Python functions using all the skills you've learned—NumPy arrays from Lecture 4, matplotlib plots from Lecture 6, file operations from Lecture 3. Then you describe these functions to Claude in a special format called a \"function schema.\" Once Claude knows about your functions, it can call them directly when answering questions.\n",
    "\n",
    "**The Function Tool Workflow:**\n",
    "1. **You define**: Write a Python function using familiar tools (just like Lecture 5)\n",
    "2. **You describe**: Create a schema that tells Claude what the function does\n",
    "3. **User asks**: Someone poses a question requiring calculation\n",
    "4. **Claude decides**: Whether to use a function based on the question\n",
    "5. **Claude requests**: Tells you which function to run with what parameters\n",
    "6. **You execute**: Run the function and send results back\n",
    "7. **Claude interprets**: Incorporates the results into a natural language response\n",
    "\n",
    "When someone asks \"What's the distance to a star with 0.05 arcsecond parallax?\", Claude recognizes this requires calculation, requests your distance function with the parameter 0.05, and then explains the result in astronomical context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Schema Concept\n",
    "\n",
    "A function schema is like a user manual for your function—it tells Claude what the function does, what parameters it needs, and when to use it. Without a schema, Claude wouldn't know your function exists or how to use it.\n",
    "\n",
    "Think of schemas as the bridge between natural language and code. When someone asks \"What's the distance to Alpha Centauri if its parallax is 0.75 arcseconds?\", the schema helps Claude understand:\n",
    "- This question needs the `parallax_to_distance` function\n",
    "- The function needs one parameter: `parallax_arcsec`\n",
    "- The value for that parameter is 0.75\n",
    "\n",
    "The schema format might look complex at first, but it's just a structured way to describe what you'd tell a colleague about your function: what it does, what inputs it needs, and what outputs it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Your First Function Tool\n",
    "\n",
    "Let's create your first function tool step by step. We'll start with the simplest possible astronomical calculation and gradually build complexity.\n",
    "\n",
    "### Setting Up the Environment\n",
    "\n",
    "First, let's import what we need. Everything here should be familiar from previous lectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Environment ready for function tools\n"
     ]
    }
   ],
   "source": [
    "# Standard imports from previous lectures\n",
    "import numpy as np  # For mathematical operations (Lecture 4)\n",
    "import os          # For environment variables (Lecture 3)\n",
    "from dotenv import load_dotenv  # For loading API keys (Lecture 7)\n",
    "import anthropic   # For talking to Claude (Lecture 7)\n",
    "\n",
    "# Load API key from .env file (same as Lecture 7)\n",
    "load_dotenv()\n",
    "client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"✔ Environment ready for function tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Astronomical Function\n",
    "\n",
    "Let's start with the most fundamental calculation in stellar astronomy: converting parallax to distance. The parallax of a star is the tiny angle it appears to shift when viewed from opposite sides of Earth's orbit. The smaller this angle, the farther away the star.\n",
    "\n",
    "The relationship is beautifully simple: distance (in parsecs) = 1 / parallax (in arcseconds). One parsec is the distance at which a star would have a parallax of exactly one arcsecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallax_to_distance(parallax_arcsec):\n",
    "    \"\"\"\n",
    "    Convert stellar parallax to distance in parsecs.\n",
    "    \n",
    "    The fundamental equation: d = 1/p\n",
    "    where d is distance in parsecs and p is parallax in arcseconds.\n",
    "    \"\"\"\n",
    "    # Input validation - always check for invalid inputs!\n",
    "    if parallax_arcsec <= 0:\n",
    "        return {\"error\": \"Parallax must be positive\"}\n",
    "    \n",
    "    # Calculate distance using the parallax formula\n",
    "    distance_pc = 1.0 / parallax_arcsec\n",
    "    \n",
    "    # Return as a dictionary for structured data\n",
    "    # We round to 2 decimal places for readability\n",
    "    return {\"distance_parsecs\": round(distance_pc, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function manually to make sure it works correctly. We'll use Proxima Centauri, our nearest stellar neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to Proxima Centauri: 1.3 parsecs\n",
      "That's about 4.2379999999999995 light-years\n",
      "\n",
      "Error handling test: {'error': 'Parallax must be positive'}\n"
     ]
    }
   ],
   "source": [
    "# Test with Proxima Centauri's parallax (0.768 arcsec)\n",
    "test_result = parallax_to_distance(0.768)\n",
    "print(f\"Distance to Proxima Centauri: {test_result['distance_parsecs']} parsecs\")\n",
    "print(f\"That's about {test_result['distance_parsecs'] * 3.26} light-years\")\n",
    "\n",
    "# Test error handling with invalid input\n",
    "error_test = parallax_to_distance(-1)\n",
    "print(f\"\\nError handling test: {error_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Function Schema\n",
    "\n",
    "Now we need to tell Claude about our function. A schema describes three key things:\n",
    "1. **The function's name** - what Claude will call it\n",
    "2. **What it does** - helps Claude know when to use it\n",
    "3. **What inputs it needs** - the parameters and their types\n",
    "\n",
    "The schema uses a specific format that might look intimidating at first, but it's just a nested dictionary structure (from Lecture 2). Let's build it step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Function schema defined\n",
      "Claude now knows about 1 function(s)\n"
     ]
    }
   ],
   "source": [
    "# Create a tools list with our function schema\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",  # The exact function name\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement in arcseconds\",\n",
    "        \"input_schema\": {  # Describes what inputs the function needs\n",
    "            \"type\": \"object\",  # The inputs are structured as an object\n",
    "            \"properties\": {  # List of parameters\n",
    "                \"parallax_arcsec\": {  # Parameter name (must match function)\n",
    "                    \"type\": \"number\",  # This parameter is a number\n",
    "                    \"description\": \"Parallax angle in arcseconds (must be positive)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]  # This parameter is mandatory\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✔ Function schema defined\")\n",
    "print(f\"Claude now knows about {len(tools)} function(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Your First Function Tool Call\n",
    "\n",
    "Now for the exciting part—let's ask Claude a question and see if it recognizes that it needs to use our function. This is different from Lecture 7 because we're giving Claude the ability to request function execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's response type: tool_use\n",
      "Number of content blocks in response: 2\n"
     ]
    }
   ],
   "source": [
    "# Ask Claude a question that requires our function\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=300,\n",
    "    tools=tools,  # NEW! This gives Claude access to our functions\n",
    "    messages=[{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the distance to a star with a parallax of 0.05 arcseconds?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# The response type tells us what Claude wants to do\n",
    "print(f\"Claude's response type: {message.stop_reason}\")\n",
    "print(f\"Number of content blocks in response: {len(message.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Tool Response Structure\n",
    "\n",
    "When Claude wants to use a tool, it doesn't just return text like in Lecture 7. Instead, it returns a structured response with multiple \"blocks.\" Some blocks contain text (Claude's thoughts), and some contain tool requests (functions Claude wants to run).\n",
    "\n",
    "Let's examine this structure carefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude wants to use a function!\n",
      "\n",
      "Block 0: Type = 'text'\n",
      "  Text content: \"I'll calculate the distance to the star using the parallax measurement you provided.\"\n",
      "Block 1: Type = 'tool_use'\n",
      "  Function to call: parallax_to_distance\n",
      "  Arguments to pass: {'parallax_arcsec': 0.05}\n",
      "  Unique ID for this call: toolu_01Hect9ikrpcZuvDnbw1XmVx\n"
     ]
    }
   ],
   "source": [
    "# Let's examine what Claude sent back\n",
    "if message.stop_reason == \"tool_use\":\n",
    "    print(\"Claude wants to use a function!\\n\")\n",
    "    \n",
    "    # Look at each block in the response\n",
    "    for i, block in enumerate(message.content):\n",
    "        print(f\"Block {i}: Type = '{block.type}'\")\n",
    "        \n",
    "        # Text blocks contain Claude's reasoning\n",
    "        if hasattr(block, 'text'):\n",
    "            print(f\"  Text content: \\\"{block.text}\\\"\")\n",
    "        \n",
    "        # Tool use blocks contain function requests\n",
    "        if hasattr(block, 'name'):\n",
    "            print(f\"  Function to call: {block.name}\")\n",
    "            print(f\"  Arguments to pass: {block.input}\")\n",
    "            print(f\"  Unique ID for this call: {block.id}\")\n",
    "else:\n",
    "    print(\"Claude responded with text only (no function needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Tool Request\n",
    "\n",
    "Claude has told us it wants to use a function, but it hasn't actually run anything yet. We need to extract the tool request, execute our Python function, and send the result back. This gives us full control over what code actually runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool request details:\n",
      "  Function name: parallax_to_distance\n",
      "  Arguments: {'parallax_arcsec': 0.05}\n",
      "  Tool ID: toolu_01Hect9ikrpcZuvDnbw1XmVx\n",
      "\n",
      "This ID is important - we need it to send results back to Claude!\n"
     ]
    }
   ],
   "source": [
    "# The tool use request is typically the last content block\n",
    "tool_use = message.content[-1]\n",
    "\n",
    "print(\"Tool request details:\")\n",
    "print(f\"  Function name: {tool_use.name}\")\n",
    "print(f\"  Arguments: {tool_use.input}\")\n",
    "print(f\"  Tool ID: {tool_use.id}\")\n",
    "print(\"\\nThis ID is important - we need it to send results back to Claude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the Function\n",
    "\n",
    "Now we need to execute our function with the arguments Claude provided. Claude sends arguments as a dictionary like `{'parallax_arcsec': 0.05}`. We can extract the value and call our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude wants to call: parallax_to_distance with {'parallax_arcsec': 0.05}\n",
      "Extracted parallax value: 0.05\n",
      "Function result: {'distance_parsecs': 20.0}\n"
     ]
    }
   ],
   "source": [
    "# Execute our function with Claude's arguments\n",
    "print(f\"Claude wants to call: {tool_use.name} with {tool_use.input}\")\n",
    "\n",
    "# Extract the parallax value from the dictionary\n",
    "parallax_value = tool_use.input['parallax_arcsec']\n",
    "print(f\"Extracted parallax value: {parallax_value}\")\n",
    "\n",
    "# Call our function\n",
    "result = parallax_to_distance(parallax_value)\n",
    "print(f\"Function result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the Conversation with Natural Language\n",
    "\n",
    "This is a crucial step: we need to send the function result back to Claude so it can formulate a complete, natural language answer. Without this step, the user would just see raw function output instead of a helpful explanation. This is what transforms a simple calculation into a conversational response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's final natural language answer:\n",
      "==================================================\n",
      "A star with a parallax of 0.05 arcseconds is at a distance of **20 parsecs** from Earth.\n",
      "\n",
      "For reference:\n",
      "- 20 parsecs = approximately 65.2 light-years\n",
      "- 20 parsecs = approximately 1.23 × 10¹⁴ miles (1.98 × 10¹⁴ kilometers)\n",
      "\n",
      "The parallax method uses the relationship that distance (in parsecs) = 1 / parallax (in arcseconds), so a parallax of 0.05 arcseconds corresponds to 1/0.05 = 20 parsecs.\n",
      "==================================================\n",
      "\n",
      "Notice how Claude converts the raw number into a complete explanation!\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation by sending the function result back to Claude\n",
    "final_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        # The original user question\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What is the distance to a star with a parallax of 0.05 arcseconds?\"\n",
    "        },\n",
    "        # Claude's response requesting the function\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": message.content\n",
    "        },\n",
    "        # Our function result sent back to Claude\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": tool_use.id,  # Must match the original request ID\n",
    "                \"content\": str(result)  # Convert result to string\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Claude's final natural language answer:\")\n",
    "print(\"=\" * 50)\n",
    "print(final_response.content[0].text)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nNotice how Claude converts the raw number into a complete explanation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building Multiple Astronomical Functions\n",
    "\n",
    "Now that you understand the complete workflow—from function definition to natural language response—let's expand your toolkit with more astronomical calculations. We'll see how Claude intelligently chooses between different functions based on the question.\n",
    "\n",
    "### Adding a Stellar Luminosity Calculator\n",
    "\n",
    "The Stefan-Boltzmann law tells us that a star's luminosity depends on its size and temperature. Specifically, L = 4πR²σT⁴, where σ is the Stefan-Boltzmann constant. This fundamental relationship lets us calculate how much energy a star emits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stellar_luminosity(radius_solar, temperature_k):\n",
    "    \"\"\"\n",
    "    Calculate stellar luminosity using the Stefan-Boltzmann law.\n",
    "    \n",
    "    The energy radiated by a star depends on its surface area (4πR²)\n",
    "    and how much energy each square meter emits (σT⁴).\n",
    "    \"\"\"\n",
    "    # Physical constants\n",
    "    stefan_boltzmann = 5.67e-8  # W m^-2 K^-4 (Stefan-Boltzmann constant)\n",
    "    solar_radius = 6.96e8  # meters (Sun's radius)\n",
    "    solar_luminosity = 3.83e26  # watts (Sun's total energy output)\n",
    "    \n",
    "    # Always validate inputs\n",
    "    if radius_solar <= 0 or temperature_k <= 0:\n",
    "        return {\"error\": \"Radius and temperature must be positive\"}\n",
    "    \n",
    "    # Convert stellar radius from solar units to meters\n",
    "    radius_meters = radius_solar * solar_radius\n",
    "    \n",
    "    # Apply Stefan-Boltzmann law: L = 4πR²σT⁴\n",
    "    luminosity_watts = 4 * np.pi * radius_meters**2 * stefan_boltzmann * temperature_k**4\n",
    "    \n",
    "    # Convert to solar luminosities for easier interpretation\n",
    "    luminosity_solar = luminosity_watts / solar_luminosity\n",
    "    \n",
    "    return {\n",
    "        \"luminosity_solar\": round(luminosity_solar, 3),\n",
    "        \"luminosity_watts\": f\"{luminosity_watts:.2e}\"  # Scientific notation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our function works correctly by testing it with the Sun's values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun's calculated luminosity: 1.004 L☉\n",
      "In watts: 3.85e+26 W\n",
      "(Should be very close to 1.0 solar luminosity!)\n",
      "\n",
      "Red giant luminosity: 84.521 L☉\n",
      "(Much brighter than the Sun despite being cooler, due to larger size)\n"
     ]
    }
   ],
   "source": [
    "# Test with the Sun (should give ~1.0 solar luminosity)\n",
    "sun_test = stellar_luminosity(1.0, 5778)  # Sun: 1 solar radius, 5778 K\n",
    "print(f\"Sun's calculated luminosity: {sun_test['luminosity_solar']} L☉\")\n",
    "print(f\"In watts: {sun_test['luminosity_watts']} W\")\n",
    "print(\"(Should be very close to 1.0 solar luminosity!)\")\n",
    "\n",
    "# Test with a red giant\n",
    "red_giant = stellar_luminosity(25, 3500)  # Typical red giant values\n",
    "print(f\"\\nRed giant luminosity: {red_giant['luminosity_solar']} L☉\")\n",
    "print(\"(Much brighter than the Sun despite being cooler, due to larger size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Tools List\n",
    "\n",
    "Now we need to tell Claude about both functions. Claude will automatically learn to choose the right function based on the question content—questions about distance will trigger the parallax function, while questions about brightness will trigger the luminosity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude now has access to 2 functions:\n",
      "  • parallax_to_distance\n",
      "  • stellar_luminosity\n"
     ]
    }
   ],
   "source": [
    "# Expanded tools list with both functions\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"parallax_arcsec\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Parallax in arcseconds (must be positive)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stellar_luminosity\", \n",
    "        \"description\": \"Calculate stellar luminosity from radius and temperature\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"radius_solar\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Stellar radius in solar radii\"\n",
    "                },\n",
    "                \"temperature_k\": {\n",
    "                    \"type\": \"number\", \n",
    "                    \"description\": \"Effective temperature in Kelvin\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"radius_solar\", \"temperature_k\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Claude now has access to {len(tools)} functions:\")\n",
    "for tool in tools:\n",
    "    print(f\"  • {tool['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Complete Tool Execution Helper\n",
    "\n",
    "Since we'll be executing tools frequently, let's create a helper function that handles the complete workflow from question to natural language answer. This will make our code cleaner, avoid repetition, and ensure we always get natural language responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool_and_respond(question, tools):\n",
    "    \"\"\"\n",
    "    Complete workflow: question → tool execution → natural language answer.\n",
    "    \n",
    "    This function handles the entire process we've been doing manually:\n",
    "    1. Send question to Claude\n",
    "    2. Execute requested function if needed\n",
    "    3. Get natural language response\n",
    "    \"\"\"\n",
    "    # Step 1: Ask Claude the question\n",
    "    initial_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=300,\n",
    "        tools=tools,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    \n",
    "    # Check if Claude wants to use a tool\n",
    "    if initial_response.stop_reason != \"tool_use\":\n",
    "        # No tool needed, return direct response\n",
    "        return initial_response.content[0].text\n",
    "    \n",
    "    # Step 2: Execute the requested function\n",
    "    tool_use = initial_response.content[-1]\n",
    "    \n",
    "    # Execute the appropriate function based on name\n",
    "    if tool_use.name == \"parallax_to_distance\":\n",
    "        # Extract the parallax value and call function\n",
    "        parallax = tool_use.input['parallax_arcsec']\n",
    "        result = parallax_to_distance(parallax)\n",
    "    elif tool_use.name == \"stellar_luminosity\":\n",
    "        # Extract both parameters and call function\n",
    "        radius = tool_use.input['radius_solar']\n",
    "        temp = tool_use.input['temperature_k']\n",
    "        result = stellar_luminosity(radius, temp)\n",
    "    else:\n",
    "        result = {\"error\": f\"Unknown function: {tool_use.name}\"}\n",
    "    \n",
    "    # Step 3: Send result back for natural language response\n",
    "    final_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=300,\n",
    "        tools=tools,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": initial_response.content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": str(result)\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return final_response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our complete workflow with different astronomical questions to see Claude choose the right tool and provide natural language answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's the distance to Proxima Centauri if its parallax is 0.768 arcseconds?\n",
      "\n",
      "Answer:\n",
      "Based on the parallax of 0.768 arcseconds, Proxima Centauri is at a distance of **1.3 parsecs**.\n",
      "\n",
      "To put this in perspective:\n",
      "- 1.3 parsecs = approximately 4.24 light-years\n",
      "- This makes Proxima Centauri the closest star to our Sun\n",
      "- 1 parsec = 3.26 light-years, so this distance is consistent with Proxima Centauri being our nearest stellar neighbor\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Question: Calculate the luminosity of Betelgeuse with radius 700 solar radii and temperature 3500 K\n",
      "\n",
      "Answer:\n",
      "The luminosity of Betelgeuse with a radius of 700 solar radii and temperature of 3,500 K is:\n",
      "\n",
      "- **66,264 solar luminosities** (L☉)\n",
      "- **2.54 × 10³¹ watts**\n",
      "\n",
      "This makes Betelgeuse an extremely luminous star - over 66,000 times more luminous than our Sun! This high luminosity is primarily due to its enormous size (700 times the Sun's radius), which more than compensates for its relatively cool surface temperature of 3,500 K (compared to the Sun's ~5,778 K).\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test different types of questions\n",
    "test_questions = [\n",
    "    \"What's the distance to Proxima Centauri if its parallax is 0.768 arcseconds?\",\n",
    "    \"Calculate the luminosity of Betelgeuse with radius 700 solar radii and temperature 3500 K\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    answer = execute_tool_and_respond(question, tools)\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to RAG (Retrieval Augmented Generation)\n",
    "\n",
    "### The Document Search Challenge\n",
    "\n",
    "After seven weeks of lectures, you've accumulated a wealth of knowledge: Python fundamentals, NumPy operations, visualization techniques, and API usage. But here's a familiar problem: when working on your research projects and you need to remember \"How did we handle errors in Lecture 3?\" or \"What was that matplotlib syntax from Lecture 6?\", you end up with twenty browser tabs open, scrolling through notebooks trying to find that one code example.\n",
    "\n",
    "This is exactly the problem that RAG (Retrieval Augmented Generation) solves. RAG combines document search with LLM reasoning, allowing you to ask questions like \"Find all the error handling techniques we learned\" and get comprehensive answers drawn directly from your course materials.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation. Think of it as giving Claude access to your personal textbook—not just its general knowledge, but your specific lecture notes and examples.\n",
    "\n",
    "The process has three steps:\n",
    "\n",
    "1. **Retrieval**: Search through your documents to find relevant sections\n",
    "2. **Augmentation**: Add those relevant sections to your question as context\n",
    "3. **Generation**: Have Claude answer using both its knowledge and your specific materials\n",
    "\n",
    "Without RAG, if you ask Claude \"What did we learn about the temperature parameter?\", it can only give general information about temperature parameters in LLMs. With RAG, it can tell you exactly what YOUR lecture notes say, with the specific examples and explanations from class.\n",
    "\n",
    "### Understanding Markdown Files (.md)\n",
    "\n",
    "Before we work with our lecture materials, let's understand what a markdown file is. You've actually been using markdown all semester—every text cell in your Jupyter notebooks uses markdown formatting!\n",
    "\n",
    "**What is a .md file?**\n",
    "A markdown file (with the extension .md) is a plain text file that uses simple symbols for formatting:\n",
    "- `#` for headers (like `# Title` or `## Section`)\n",
    "- `*` for italics and `**` for bold\n",
    "- ``` for code blocks\n",
    "- `-` for bullet points\n",
    "\n",
    "The beauty of markdown is that it's human-readable even without rendering. You can open a .md file in any text editor (like Cursor, Notepad, or TextEdit) and read it easily.\n",
    "\n",
    "### Converting Jupyter Notebooks to Markdown with Jupytext\n",
    "\n",
    "Your lecture materials are currently in Jupyter notebook format (.ipynb files), which contain both code and text mixed with metadata and output. To make them searchable for RAG, we need to convert them to plain markdown.\n",
    "\n",
    "**Jupytext** is a tool that converts between different notebook formats. Think of it as a translator that can turn your .ipynb files into clean .md files. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupytext in ./anaconda3/lib/python3.13/site-packages (1.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=1.0 in ./anaconda3/lib/python3.13/site-packages (from jupytext) (2.2.0)\n",
      "Requirement already satisfied: mdit-py-plugins in ./anaconda3/lib/python3.13/site-packages (from jupytext) (0.3.0)\n",
      "Requirement already satisfied: nbformat in ./anaconda3/lib/python3.13/site-packages (from jupytext) (5.10.4)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.13/site-packages (from jupytext) (24.2)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.13/site-packages (from jupytext) (6.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./anaconda3/lib/python3.13/site-packages (from markdown-it-py>=1.0->jupytext) (0.1.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./anaconda3/lib/python3.13/site-packages (from nbformat->jupytext) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./anaconda3/lib/python3.13/site-packages (from nbformat->jupytext) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./anaconda3/lib/python3.13/site-packages (from nbformat->jupytext) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./anaconda3/lib/python3.13/site-packages (from nbformat->jupytext) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./anaconda3/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat->jupytext) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./anaconda3/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat->jupytext) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./anaconda3/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat->jupytext) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./anaconda3/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat->jupytext) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./anaconda3/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->jupytext) (4.3.7)\n",
      "Jupytext installed!\n"
     ]
    }
   ],
   "source": [
    "# First, install Jupytext if you haven't already\n",
    "!pip install jupytext\n",
    "print(\"Jupytext installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert your notebook files to markdown, you would use Jupytext from the terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading Lecture7_LLM_API_Basics_20250913.ipynb in format ipynb\n",
      "[jupytext] Updating the timestamp of Lecture7_LLM_API_Basics_20250913.md\n"
     ]
    }
   ],
   "source": [
    "!jupytext --to md Lecture7_LLM_API_Basics_20250913.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This creates a file called `Lecture7_LLM_API_Basics_20250913.md` in the same folder. The markdown file contains all your text cells and code cells from the notebook, but in a clean text format perfect for searching.\n",
    "\n",
    "For this lecture, we've already converted Lecture 7 to markdown format, so we can work with it directly. Let's examine this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File contains 63,566 characters\n",
      "That's approximately 8,440 words\n",
      "Roughly equivalent to 28 pages of text\n",
      "\n",
      "First 500 characters of the markdown file:\n",
      "==================================================\n",
      "---\n",
      "jupyter:\n",
      "  jupytext:\n",
      "    text_representation:\n",
      "      extension: .md\n",
      "      format_name: markdown\n",
      "      format_version: '1.3'\n",
      "      jupytext_version: 1.17.2\n",
      "  kernelspec:\n",
      "    display_name: base\n",
      "    language: python\n",
      "    name: python3\n",
      "---\n",
      "\n",
      "# Large Language Model APIs\n",
      "\n",
      "*Tutorial by Yuan-Sen Ting*\n",
      "\n",
      "*Astron 1221: Lecture 7*\n",
      "\n",
      "\n",
      "## Introduction: Your Code Meets Large Language Models\n",
      "\n",
      "### The Moment Everything Clicks Together\n",
      "\n",
      "For the past six weeks, you've been building a programming foundation. Variab\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Read the pre-converted lecture file\n",
    "with open('Lecture7_LLM_API_Basics_20250913.md', 'r') as f:\n",
    "    lecture7_content = f.read()\n",
    "\n",
    "# Check the size\n",
    "print(f\"File contains {len(lecture7_content):,} characters\")\n",
    "print(f\"That's approximately {len(lecture7_content.split()):,} words\")\n",
    "print(f\"Roughly equivalent to {len(lecture7_content.split())//300} pages of text\")\n",
    "\n",
    "# Look at the beginning to see the structure\n",
    "print(\"\\nFirst 500 characters of the markdown file:\")\n",
    "print(\"=\" * 50)\n",
    "print(lecture7_content[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Section Headers in the Document\n",
    "\n",
    "Markdown uses `#` symbols for headers. In our lecture file:\n",
    "- `#` marks the main title\n",
    "- `##` marks major sections\n",
    "- `###` marks subsections\n",
    "\n",
    "Let's find all the main topics covered in Lecture 7 using simple string methods you learned in Lecture 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 main sections in Lecture 7:\n",
      "\n",
      "  1. Introduction: Your Code Meets Large Language Models\n",
      "  2. Part 1: Understanding APIs\n",
      "  3. Part 2: Setting Up Your Connection\n",
      "  4. Part 3: Your First API Call\n",
      "  5. Part 4: Understanding the Parameters\n",
      "  6. Part 5: Building Conversations\n",
      "  7. Part 6: Prompting Strategies\n",
      "  8. Part 7: Making It Practical\n",
      "  ... and 3 more sections\n"
     ]
    }
   ],
   "source": [
    "# Find all main sections using string methods\n",
    "sections = []\n",
    "lines = lecture7_content.split('\\n')  # Split into individual lines\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line starts with '## ' (main section header)\n",
    "    if line.startswith('## '):\n",
    "        # Remove the '## ' to get just the title\n",
    "        section_title = line[3:]  # Everything after '## '\n",
    "        sections.append(section_title)\n",
    "\n",
    "print(f\"Found {len(sections)} main sections in Lecture 7:\")\n",
    "print()\n",
    "for i, section in enumerate(sections[:8], 1):  # Show first 8\n",
    "    print(f\"  {i}. {section}\")\n",
    "if len(sections) > 8:\n",
    "    print(f\"  ... and {len(sections) - 8} more sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Document Chunking\n",
    "\n",
    "### Why We Need to Chunk Documents\n",
    "\n",
    "Our Lecture 7 file contains tens of thousands of characters—that's enormous! Sending the entire document to Claude every time we ask a question would create three major problems:\n",
    "\n",
    "1. **Cost**: We'd be paying for all those characters as input tokens for every single question, even if we're only asking about one small topic\n",
    "2. **Relevance**: If you ask about \"API errors\", 95% of the document isn't relevant—it's about other topics like image processing or conversation management\n",
    "3. **Focus**: Claude performs better with focused, relevant context rather than being overwhelmed with unrelated information\n",
    "\n",
    "The solution is **document chunking**—breaking the large document into smaller, manageable pieces. Think of it like organizing a library: instead of reading every page of every book to answer a question, you first identify which chapter or section is most relevant.\n",
    "\n",
    "### Simple Section-Based Chunking\n",
    "\n",
    "The simplest chunking strategy is to split by section headers. Each section of the lecture becomes its own searchable chunk. This works well for structured documents like lecture notes where each section covers a specific topic.\n",
    "\n",
    "Let's implement this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sections(text):\n",
    "    \"\"\"\n",
    "    Split a document into chunks based on ## section headers.\n",
    "    \n",
    "    This function:\n",
    "    1. Finds all the ## headers in the text\n",
    "    2. Splits the document at these headers\n",
    "    3. Keeps each section as a separate chunk\n",
    "    4. Preserves the section header with its content\n",
    "    \"\"\"\n",
    "    # Split on section headers\n",
    "    # We use '\\n## ' to ensure we're splitting on headers at line starts\n",
    "    sections = text.split('\\n## ')\n",
    "    \n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        # The first section doesn't have '## ' removed (it wasn't split)\n",
    "        if i == 0:\n",
    "            chunk_text = section\n",
    "        else:\n",
    "            # Add back the '## ' that was removed during split\n",
    "            chunk_text = '## ' + section\n",
    "        \n",
    "        # Only keep chunks with substantial content (at least 100 characters)\n",
    "        if len(chunk_text.strip()) > 100:\n",
    "            chunks.append({\n",
    "                'text': chunk_text.strip(),\n",
    "                'length': len(chunk_text),\n",
    "                'chunk_id': i\n",
    "            })\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12 chunks from Lecture 7\n",
      "\n",
      "Chunk statistics:\n",
      "  Average size: 5,296 characters\n",
      "  Smallest: 323 characters\n",
      "  Largest: 9,631 characters\n"
     ]
    }
   ],
   "source": [
    "# Create chunks from our lecture\n",
    "lecture_chunks = chunk_by_sections(lecture7_content)\n",
    "\n",
    "print(f\"Created {len(lecture_chunks)} chunks from Lecture 7\")\n",
    "print(f\"\\nChunk statistics:\")\n",
    "print(f\"  Average size: {sum(c['length'] for c in lecture_chunks) // len(lecture_chunks):,} characters\")\n",
    "print(f\"  Smallest: {min(c['length'] for c in lecture_chunks):,} characters\")\n",
    "print(f\"  Largest: {max(c['length'] for c in lecture_chunks):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what our chunks look like to understand what we've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 chunks from Lecture 7:\n",
      "==================================================\n",
      "\n",
      "Chunk 0:\n",
      "  Title: ---\n",
      "  Size: 323 characters\n",
      "  Preview: ---\n",
      "jupyter:\n",
      "  jupytext:\n",
      "    text_representation:\n",
      "      extension: .md\n",
      "      format_name: markdown\n",
      "      format_version: '1.3'\n",
      "      jupytext_version:...\n",
      "\n",
      "Chunk 1:\n",
      "  Title: ## Introduction: Your Code Meets Large Language Models\n",
      "  Size: 6,095 characters\n",
      "  Preview: ## Introduction: Your Code Meets Large Language Models\n",
      "\n",
      "### The Moment Everything Clicks Together\n",
      "\n",
      "For the past six weeks, you've been building a prog...\n",
      "\n",
      "Chunk 2:\n",
      "  Title: ## Part 1: Understanding APIs\n",
      "  Size: 3,355 characters\n",
      "  Preview: ## Part 1: Understanding APIs\n",
      "\n",
      "### What Is an API, Really?\n",
      "\n",
      "Let's demystify this term that gets thrown around constantly in programming. API stands fo...\n"
     ]
    }
   ],
   "source": [
    "# Examine the first few chunks\n",
    "print(\"First 3 chunks from Lecture 7:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(lecture_chunks))):\n",
    "    chunk = lecture_chunks[i]\n",
    "    # Get the first line (usually the section title)\n",
    "    first_line = chunk['text'].split('\\n')[0]\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Title: {first_line}\")\n",
    "    print(f\"  Size: {chunk['length']:,} characters\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Overlapping Chunks\n",
    "\n",
    "A potential problem with simple splitting: what if important information spans across chunk boundaries? Imagine reading a textbook where each chapter ends mid-sentence—you'd lose crucial context!\n",
    "\n",
    "**Overlapping chunks** solve this by having each chunk include some content from its neighbors. It's like having each chapter of a book reprint the last paragraph of the previous chapter and the first paragraph of the next chapter. This ensures nothing important gets lost in the gaps between chunks.\n",
    "\n",
    "Here's a visual example:\n",
    "```\n",
    "Original text: \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "Non-overlapping chunks of size 10:\n",
    "  Chunk 1: \"ABCDEFGHIJ\"\n",
    "  Chunk 2: \"KLMNOPQRST\"\n",
    "  Chunk 3: \"UVWXYZ\"\n",
    "  \n",
    "Overlapping chunks (size 10, overlap 3):\n",
    "  Chunk 1: \"ABCDEFGHIJ\"\n",
    "  Chunk 2: \"HIJKLMNOPQ\"  (starts at H, overlaps HIJ)\n",
    "  Chunk 3: \"OPQRSTUVWX\"  (starts at O, overlaps OPQ)\n",
    "```\n",
    "\n",
    "While overlapping chunks are more sophisticated and useful for many applications, for our lecture materials that are already well-structured with clear section boundaries, simple section-based chunking works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Understanding Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Now we have chunks of text, but how do we find which chunks are relevant to a user's question? We can't just search for exact word matches—what if someone asks about \"error handling\" but the text says \"exception management\"? These mean the same thing but use different words.\n",
    "\n",
    "This is where **embeddings** come in. An embedding is a way to convert text into a list of numbers (called a vector) that captures the semantic meaning of that text. The key insight: texts with similar meanings will have similar number patterns, even if they use different words.\n",
    "\n",
    "Think of it like this:\n",
    "- \"stellar parallax\" might become [0.2, -0.1, 0.8, 0.3, ..., 0.5] (384 numbers)\n",
    "- \"star distance measurement\" might become [0.3, -0.2, 0.7, 0.4, ..., 0.4] (384 numbers)\n",
    "- \"cooking recipes\" might become [0.9, 0.5, -0.3, 0.1, ..., -0.2] (384 numbers)\n",
    "\n",
    "Notice how the first two (both about measuring star distances) have similar number patterns, while the third (about cooking) is completely different. The embedding model has learned that \"parallax\" and \"distance measurement\" are related concepts in astronomy.\n",
    "\n",
    "### How Embeddings Capture Meaning\n",
    "\n",
    "Embedding models are neural networks trained on millions of documents. Through this training, they learn:\n",
    "- \"API\" and \"programming interface\" are related concepts\n",
    "- \"error\" and \"exception\" often mean similar things in programming\n",
    "- \"temperature\" in the context of LLMs is different from \"temperature\" in physics\n",
    "\n",
    "Each dimension in the embedding vector captures some aspect of meaning. While we can't interpret what each individual number means (they're learned by the neural network), we can measure how similar two embeddings are to find related texts.\n",
    "\n",
    "### Important Note: Normalized Embeddings\n",
    "\n",
    "Most modern embedding models, including the one we'll use, output **normalized vectors**. This means all embedding vectors have a magnitude (length) of 1.0. This is a crucial property that simplifies our calculations significantly!\n",
    "\n",
    "### Important Tip: Complete Sentences Give Better Embeddings\n",
    "\n",
    "When creating embeddings, **complete sentences often work better than keywords!** The embedding model can better understand context and meaning from full sentences. For example:\n",
    "- \"How to measure stellar parallax?\" gives richer embeddings than just \"parallax\"\n",
    "- \"What are the error handling techniques in Python?\" is better than \"error handling\"\n",
    "\n",
    "This is because the model was trained on natural language text, so it better understands the relationships between words when they appear in complete thoughts.\n",
    "\n",
    "### Measuring Similarity with Cosine Similarity\n",
    "\n",
    "Once we have embeddings (vectors of numbers), we need to measure how similar they are. We use **cosine similarity**, which measures the angle between two vectors.\n",
    "\n",
    "The intuition is simple:\n",
    "- Vectors pointing in the same direction = similar meaning (cosine similarity ≈ 1)\n",
    "- Vectors at right angles = unrelated (cosine similarity ≈ 0)\n",
    "- Vectors pointing opposite ways = opposite meanings (cosine similarity ≈ -1)\n",
    "\n",
    "The mathematical formula is: cosine(θ) = (A·B) / (||A|| × ||B||)\n",
    "\n",
    "Where:\n",
    "- A·B is the dot product (measures alignment)\n",
    "- ||A|| and ||B|| are the vector magnitudes (lengths)\n",
    "\n",
    "**However, since embedding models output normalized vectors (||A|| = ||B|| = 1), the formula simplifies to just the dot product: cosine(θ) = A·B**\n",
    "\n",
    "Let's implement the simplified version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity for normalized vectors.\n",
    "    Since ||vec1|| = ||vec2|| = 1, cosine similarity = dot product.\n",
    "    Much faster and simpler!\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Sentence-Transformers Library\n",
    "\n",
    "To create actual embeddings that capture semantic meaning, we'll use a library called **sentence-transformers**. This library provides pre-trained neural network models that can convert any text into meaningful embedding vectors.\n",
    "\n",
    "**What does sentence-transformers do?**\n",
    "- Provides ready-to-use embedding models trained on millions of documents\n",
    "- Handles all the complex neural network operations behind the scenes\n",
    "- Converts text to vectors that actually capture semantic meaning\n",
    "- Works with sentences, paragraphs, or entire documents\n",
    "\n",
    "We'll use a model called **'all-MiniLM-L6-v2'** for this tutorial. Breaking down this name helps understand what we're working with: \"all\" means it works for all types of English text, \"MiniLM\" indicates it's a smaller, faster version of a language model, \"L6\" tells us it has 6 layers (the depth of the neural network), and \"v2\" simply means it's version 2, improved from the original.\n",
    "\n",
    "**Importantly, this model outputs normalized vectors**, so we can use the simplified cosine similarity calculation.\n",
    "\n",
    "This model converts any text into 384 numbers that capture its meaning. Through its training, it has learned that phrases like \"stellar distance\" and \"how far away is the star\" mean similar things, even though they use different words. We're using this particular model because it strikes the perfect balance for learning—it's small enough to run quickly on any computer (only 80MB download), fast enough for interactive experimentation, and powerful enough to demonstrate all RAG concepts effectively.\n",
    "\n",
    "**More Powerful Models in Production**\n",
    "\n",
    "In professional research and production systems, you'll often encounter more sophisticated embedding models. For example, OpenAI's text-embedding-3-large creates 3,072-dimensional embeddings compared to our 384, providing much richer semantic understanding. Google's text-embedding-004 produces 768-dimensional embeddings with excellent multilingual support. Specialized models like Voyage AI's voyage-3 or Cohere's embed-v3 offer 1,024 dimensions optimized for domain-specific or technical texts.\n",
    "\n",
    "These larger models can capture more subtle semantic relationships and often perform better with specialized scientific literature. However, they come with trade-offs: they're more expensive (often requiring API payments), slower to run, require significantly more memory and storage, and are honestly overkill for learning the fundamental concepts.\n",
    "\n",
    "Think of it like choosing a telescope: our all-MiniLM-L6-v2 is like a reliable 8-inch telescope that's perfect for learning astronomy. The production models are like research-grade observatories—more powerful, but you don't need them to understand how telescopes work! For your course projects and learning RAG concepts, our smaller model is perfectly adequate. When you eventually move to research-scale projects with thousands of papers, you can upgrade to these more powerful models using the exact same techniques you're learning today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-transformers library installed!\n"
     ]
    }
   ],
   "source": [
    "# Install the sentence-transformers library\n",
    "!pip install -q sentence-transformers\n",
    "print(\"Sentence-transformers library installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "(First time will download ~80MB model file)\n",
      "✔ Model loaded successfully!\n",
      "\n",
      "Model information:\n",
      "  Output dimensions: 384\n",
      "  Max input length: 256 tokens\n",
      "  (A token is roughly a word or word piece)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "print(\"(First time will download ~80MB model file)\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✔ Model loaded successfully!\")\n",
    "\n",
    "# Explore model properties\n",
    "print(f\"\\nModel information:\")\n",
    "print(f\"  Output dimensions: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max input length: {embedding_model.max_seq_length} tokens\")\n",
    "print(f\"  (A token is roughly a word or word piece)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the embedding model to see how it captures semantic meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for test phrases...\n",
      "  'stellar parallax measurement': vector with 384 dimensions\n",
      "  'measuring star distances': vector with 384 dimensions\n",
      "  'galaxy classification': vector with 384 dimensions\n",
      "  'cooking recipes': vector with 384 dimensions\n",
      "\n",
      "Checking if embeddings are normalized:\n",
      "  'stellar parallax measurement': norm = 1.0000\n",
      "  'measuring star distances': norm = 1.0000\n",
      "  'galaxy classification': norm = 1.0000\n",
      "  'cooking recipes': norm = 1.0000\n",
      "\n",
      "✓ All embeddings are normalized! We can use the simplified dot product for similarity.\n"
     ]
    }
   ],
   "source": [
    "# Test with astronomy concepts\n",
    "test_texts = [\n",
    "    \"stellar parallax measurement\",\n",
    "    \"measuring star distances\",  # Similar meaning, different words\n",
    "    \"galaxy classification\",      # Different astronomy topic\n",
    "    \"cooking recipes\"             # Completely unrelated\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Creating embeddings for test phrases...\")\n",
    "test_embeddings = []\n",
    "for text in test_texts:\n",
    "    embedding = embedding_model.encode(text)\n",
    "    test_embeddings.append(embedding)\n",
    "    print(f\"  '{text}': vector with {len(embedding)} dimensions\")\n",
    "\n",
    "# Verify that embeddings are normalized\n",
    "print(\"\\nChecking if embeddings are normalized:\")\n",
    "for text, embedding in zip(test_texts, test_embeddings):\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    print(f\"  '{text}': norm = {norm:.4f}\")\n",
    "\n",
    "print(\"\\n✓ All embeddings are normalized! We can use the simplified dot product for similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic similarities between phrases:\n",
      "==================================================\n",
      "'stellar parallax measurement' vs 'measuring star distances'\n",
      "  Similarity: 0.622\n",
      "'stellar parallax measurement' vs 'galaxy classification'\n",
      "  Similarity: 0.333\n",
      "'stellar parallax measurement' vs 'cooking recipes'\n",
      "  Similarity: 0.020\n",
      "'measuring star distances' vs 'galaxy classification'\n",
      "  Similarity: 0.310\n",
      "'measuring star distances' vs 'cooking recipes'\n",
      "  Similarity: 0.048\n",
      "'galaxy classification' vs 'cooking recipes'\n",
      "  Similarity: 0.107\n",
      "\n",
      "Notice: 'stellar parallax' and 'star distances' have HIGH similarity!\n",
      "The model understands they're about the same concept.\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarities between all pairs\n",
    "print(\"\\nSemantic similarities between phrases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(len(test_texts)):\n",
    "    for j in range(i+1, len(test_texts)):\n",
    "        sim = cosine_similarity(test_embeddings[i], test_embeddings[j])\n",
    "        print(f\"'{test_texts[i]}' vs '{test_texts[j]}'\")\n",
    "        print(f\"  Similarity: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nNotice: 'stellar parallax' and 'star distances' have HIGH similarity!\")\n",
    "print(\"The model understands they're about the same concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Building the Complete RAG System\n",
    "\n",
    "Now let's combine everything we've learned to build a complete RAG system. We'll create embeddings for all our lecture chunks, build a search function that finds relevant content, and use that content to answer questions.\n",
    "\n",
    "### Step 1: Create Embeddings for All Chunks\n",
    "\n",
    "First, we need to convert every chunk of our lecture into an embedding vector. This is like creating an index for a book—we're preparing the content to be efficiently searchable. Each chunk gets converted to 384 numbers that capture its meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 12 chunks...\n",
      "This may take a minute...\n",
      "\n",
      "  Processed 5/12 chunks\n",
      "  Processed 10/12 chunks\n",
      "\n",
      "✔ Created 12 embeddings\n",
      "Each embedding has 384 dimensions\n",
      "Total data: 12 chunks × 384 dimensions = 4,608 numbers\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all lecture chunks\n",
    "print(f\"Creating embeddings for {len(lecture_chunks)} chunks...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "chunk_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(lecture_chunks):\n",
    "    # Create embedding for this chunk's text\n",
    "    # The encode() method converts text to a vector\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    chunk_embeddings.append(embedding)\n",
    "    \n",
    "    # Show progress every 5 chunks\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(lecture_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n✔ Created {len(chunk_embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(chunk_embeddings[0])} dimensions\")\n",
    "print(f\"Total data: {len(chunk_embeddings)} chunks × {len(chunk_embeddings[0])} dimensions = {len(chunk_embeddings) * len(chunk_embeddings[0]):,} numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the Search Function\n",
    "\n",
    "Now we can build a search function that finds the most relevant chunks for any question. This is the \"Retrieval\" part of RAG. The process is:\n",
    "1. Convert the user's question to an embedding\n",
    "2. Compare it with all chunk embeddings using cosine similarity\n",
    "3. Return the chunks with the highest similarity scores\n",
    "\n",
    "This is like having a librarian who understands meaning, not just keywords. If you ask about \"error handling\", it will find sections about \"exceptions\" and \"try-except blocks\" even if they don't use the exact phrase \"error handling\".\n",
    "\n",
    "We'll use a vectorized approach for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most relevant chunks for a query using vectorized operations.\n",
    "    \n",
    "    This function:\n",
    "    1. Converts the query to an embedding (384 numbers)\n",
    "    2. Calculates similarity with all chunk embeddings using vectorized NumPy\n",
    "    3. Returns the top-k most similar chunks\n",
    "    \n",
    "    Parameters:\n",
    "    - query: The search question\n",
    "    - top_k: How many results to return\n",
    "    \"\"\"\n",
    "    # Convert query to embedding (same 384-dimensional space as chunks)\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Vectorized similarity calculation - much faster than a loop!\n",
    "    # Convert list of embeddings to NumPy array for vectorized operations\n",
    "    chunk_matrix = np.array(chunk_embeddings)\n",
    "    \n",
    "    # Calculate dot products with all chunks at once\n",
    "    similarities = np.dot(chunk_matrix, query_embedding)\n",
    "    \n",
    "    # Find the indices of top-k highest similarities\n",
    "    # argsort() returns indices that would sort the array\n",
    "    # [-top_k:] takes the last k elements (highest values)\n",
    "    # [::-1] reverses to get descending order\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Return the top chunks with their similarities\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': lecture_chunks[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing the Search\n",
    "\n",
    "Let's test our search function with a specific question about API security from Lecture 7. This will show us which sections of the lecture are most relevant to our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I keep API keys secure?'\n",
      "\n",
      "==================================================\n",
      "Found 2 relevant sections:\n",
      "==================================================\n",
      "\n",
      "Result 1:\n",
      "  Similarity score: 0.361\n",
      "  (1.0 = perfect match, 0.0 = unrelated)\n",
      "  Section: ## Part 1: Understanding APIs\n",
      "  Preview: ## Part 1: Understanding APIs\n",
      "\n",
      "### What Is an API, Really?\n",
      "\n",
      "Let's demystify this term that gets thrown around constantly in programming. API stands for Application Programming Interface, but that defi...\n",
      "\n",
      "Result 2:\n",
      "  Similarity score: 0.329\n",
      "  (1.0 = perfect match, 0.0 = unrelated)\n",
      "  Section: ## Part 2: Setting Up Your Connection\n",
      "  Preview: ## Part 2: Setting Up Your Connection\n",
      "\n",
      "### Getting Your API Key\n",
      "\n",
      "Before we write any code, you need to obtain an API key from an LLM provider. We'll focus on Anthropic's Claude for this lecture becaus...\n"
     ]
    }
   ],
   "source": [
    "# Test search with a specific question\n",
    "query = \"How do I keep API keys secure?\"\n",
    "results = search_chunks(query, top_k=2)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Found {len(results)} relevant sections:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    # Extract section title (first line)\n",
    "    lines = result['chunk']['text'].split('\\n')\n",
    "    title = lines[0] if lines else \"No title\"\n",
    "    \n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Similarity score: {result['similarity']:.3f}\")\n",
    "    print(f\"  (1.0 = perfect match, 0.0 = unrelated)\")\n",
    "    print(f\"  Section: {title}\")\n",
    "    print(f\"  Preview: {result['chunk']['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: RAG-Powered Question Answering\n",
    " \n",
    "Now for the complete RAG workflow. We'll create a function that combines everything:\n",
    " \n",
    "1. **Retrieval**: Search for relevant chunks from our lecture materials using semantic similarity\n",
    "2. **Augmentation**: Add the retrieved content to our prompt as context for the AI\n",
    "3. **Generation**: Use Claude to generate an answer based on the retrieved information\n",
    " \n",
    "The `rag_answer()` function below implements this complete pipeline:\n",
    " \n",
    "- **Input**: Takes a question and optionally the number of chunks to retrieve\n",
    "- **Retrieval Step**: Uses our `search_chunks()` function to find the most relevant sections\n",
    "- **Quality Check**: Filters out results with low similarity scores (< 0.2) to avoid irrelevant content\n",
    "- **Smart Augmentation**: Combines retrieved chunks but ensures we end at complete sentences (no cut-off mid-sentence)\n",
    "- **Prompt Engineering**: Creates a structured prompt that includes both the question and retrieved course materials\n",
    "- **Generation**: Sends the augmented prompt to Claude with low temperature (0.0) for factual accuracy\n",
    "- **Output**: Returns an answer grounded in our actual course materials\n",
    " \n",
    "This approach ensures the AI answers questions using specific information from our course materials, rather than just relying on its general training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question, max_chunks=2):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval Augmented Generation).\n",
    "    \n",
    "    Improved version that doesn't cut off mid-sentence!\n",
    "    \n",
    "    The three RAG steps:\n",
    "    1. RETRIEVAL: Find relevant chunks from course materials\n",
    "    2. AUGMENTATION: Add those chunks to the prompt\n",
    "    3. GENERATION: Get Claude to answer using the retrieved content\n",
    "    \"\"\"\n",
    "    print(f\"Searching for content related to: '{question}'\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    results = search_chunks(question, top_k=max_chunks)\n",
    "    \n",
    "    # Check if we found relevant content\n",
    "    if results[0]['similarity'] < 0.2:\n",
    "        return \"No relevant content found in course materials for this question.\"\n",
    "    \n",
    "    print(f\"Found {len(results)} relevant sections (similarity > 0.2)\")\n",
    "    \n",
    "    # Step 2: Augment - combine retrieved chunks \n",
    "    context_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Take more content but end at a complete sentence\n",
    "        chunk_text = result['chunk']['text'][:1500]  # Take up to 1500 chars\n",
    "        \n",
    "        # Find the last period, question mark, or exclamation point\n",
    "        # to end at a complete sentence\n",
    "        last_sentence_end = max(\n",
    "            chunk_text.rfind('.'),\n",
    "            chunk_text.rfind('?'),\n",
    "            chunk_text.rfind('!')\n",
    "        )\n",
    "        \n",
    "        if last_sentence_end > 0:\n",
    "            chunk_text = chunk_text[:last_sentence_end + 1]\n",
    "        \n",
    "        context_parts.append(f\"Section {i}:\\n{chunk_text}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create augmented prompt with retrieved content\n",
    "    augmented_prompt = f\"\"\"Based on the following course materials from Lecture 7, answer this question: {question}\n",
    "\n",
    "COURSE MATERIALS:\n",
    "{context}\n",
    "\n",
    "Please provide a comprehensive answer based specifically on what the course materials say. Use the exact terminology and examples from the lecture.\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer with Claude\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=400,\n",
    "        temperature=0.0,  # Low temperature for factual accuracy\n",
    "        messages=[{\"role\": \"user\", \"content\": augmented_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG-POWERED ANSWER\n",
      "======================================================================\n",
      "Searching for content related to: 'What are the main parameters for API calls we learned about?'\n",
      "Found 2 relevant sections (similarity > 0.2)\n",
      "\n",
      "Answer based on Lecture 7 content:\n",
      "Based on the course materials from Lecture 7, the main parameters for API calls that we learned about are:\n",
      "\n",
      "## Core API Call Parameters\n",
      "\n",
      "The course materials show three main parameters when making API calls to Claude:\n",
      "\n",
      "### 1. **model**\n",
      "- Specifies which model to use for the request\n",
      "- Example from the materials: `\"claude-sonnet-4-20250514\"`\n",
      "- This parameter tells the API which specific version of the LLM you want to communicate with\n",
      "\n",
      "### 2. **max_tokens** \n",
      "- Controls the maximum response length\n",
      "- Example from the materials: `100`\n",
      "- This parameter limits how long Claude's response can be\n",
      "\n",
      "### 3. **messages**\n",
      "- Contains the actual conversation content\n",
      "- Structured as a list of dictionaries with \"role\" and \"content\" keys\n",
      "- Example from the materials:\n",
      "```python\n",
      "messages=[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What type of galaxy is M31?\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "## How These Parameters Work Together\n",
      "\n",
      "The course materials explain that when you make an API call, you're sending \"a specially formatted message over the internet to the LLM's servers. The message contains your prompt, along with parameters like which model to use and how long the response should be.\"\n",
      "\n",
      "The materials emphasize that these parameters are passed to the `client.messages.create()` method, where the client object serves as \"your gateway to Claude\" that \"handles authentication, manages the network connection, and provides methods for sending messages.\"\n",
      "\n",
      "These three parameters—model, max_tokens, and messages—represent the essential information needed to structure the conversation between your Python code and the LLM, following the restaurant analogy where you need to specify what you want (messages), from which kitchen (model), and how much food you want (max_tokens\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test RAG answering with a question about course content\n",
    "test_question = \"What are the main parameters for API calls we learned about?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAG-POWERED ANSWER\")\n",
    "print(\"=\" * 70)\n",
    "answer = rag_answer(test_question)\n",
    "print(f\"\\nAnswer based on Lecture 7 content:\")\n",
    "print(answer)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RAG vs Non-RAG Responses\n",
    "\n",
    "Let's see the dramatic difference between Claude's general knowledge and answers grounded in your specific course materials. This demonstrates why RAG is so powerful for working with your own documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT Course Materials (General Knowledge):\n",
      "==================================================\n",
      "I don't have the specific context of which API you're referring to, but I can share some general insights about what's typically learned regarding conversation histories in APIs:\n",
      "\n",
      "## Common Learnings About Conversation Histories:\n",
      "\n",
      "**State Management:**\n",
      "- APIs often need to maintain conversation context across multiple requests\n",
      "- Stateless vs. stateful approaches have different tradeoffs\n",
      "\n",
      "**Data Structure:**\n",
      "- Conversation histories are usually structured as arrays of message objects\n",
      "- Each message typically includes role (user/assistant/system), content, and metadata\n",
      "\n",
      "**Performance Considerations:**\n",
      "- Long conversation histories can impact response time and token usage\n",
      "- Truncation or summarization strategies may be needed\n",
      "\n",
      "**Context Window Limitations:**\n",
      "- Most AI APIs have maximum context lengths\n",
      "- Older messages may need to be pruned or compressed\n",
      "\n",
      "**Memory and Persistence:**\n",
      "- APIs may or may not persist conversation history between sessions\n",
      "- Client-side vs. server-\n",
      "\n",
      "======================================================================\n",
      "\n",
      "WITH Course Materials (RAG-Enhanced):\n",
      "==================================================\n",
      "Searching for content related to: 'What did we learn about conversation histories in the API?'\n",
      "Found 2 relevant sections (similarity > 0.2)\n",
      "Based on the course materials from Lecture 7, here's what we learned about conversation histories in the API:\n",
      "\n",
      "## The API is Completely Stateless\n",
      "\n",
      "The most important concept we learned is that **the API is completely stateless**—each API call is independent and has no memory of previous calls. This is fundamentally different from the chat interface where Claude seems to \"remember\" your conversation.\n",
      "\n",
      "## You Must Explicitly Provide Conversation History\n",
      "\n",
      "To maintain context across multiple API calls, you must explicitly provide the entire conversation history with each request. The API doesn't automatically remember what was said before.\n",
      "\n",
      "## Demonstration of the Problem\n",
      "\n",
      "The course materials provided a clear example showing this limitation:\n",
      "\n",
      "**Call 1:**\n",
      "```python\n",
      "response1 = client.messages.create(\n",
      "    model=\"claude-sonnet-4-20250514\",\n",
      "    max_tokens=100,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"My favorite galaxy is M31. Remember this.\"}]\n",
      ")\n",
      "```\n",
      "\n",
      "**Call 2 (separate and independent):**\n",
      "```python\n",
      "response2 = client.messages.create(\n",
      "    model=\"claude-sonnet-4-20250514\",\n",
      "    max_tokens=100,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"What is my favorite galaxy?\"}]\n",
      ")\n",
      "```\n",
      "\n",
      "The key learning point emphasized in the materials is: **\"Notice: Claude has no idea! Each API call is completely independent.\"**\n",
      "\n",
      "## The Structure of Messages\n",
      "\n",
      "We also learned that conversations are structured using a `messages` array, where each message has a `\"role\"` (like \"user\") and `\"content\"` containing the actual message text.\n",
      "\n",
      "This stateless nature means that if you want to build a conversation, you need to manually track and include the entire conversation history in each subsequent API call\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Notice: The RAG answer references specific details from YOUR lecture!\n",
      "It mentions the exact concepts and examples we covered in class.\n"
     ]
    }
   ],
   "source": [
    "comparison_question = \"What did we learn about conversation histories in the API?\"\n",
    "\n",
    "# Without RAG - just Claude's general knowledge\n",
    "print(\"WITHOUT Course Materials (General Knowledge):\")\n",
    "print(\"=\" * 50)\n",
    "general_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    messages=[{\"role\": \"user\", \"content\": comparison_question}]\n",
    ")\n",
    "print(general_response.content[0].text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# With RAG - using course materials\n",
    "print(\"WITH Course Materials (RAG-Enhanced):\")\n",
    "print(\"=\" * 50)\n",
    "rag_answer_text = rag_answer(comparison_question)\n",
    "print(rag_answer_text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nNotice: The RAG answer references specific details from YOUR lecture!\")\n",
    "print(\"It mentions the exact concepts and examples we covered in class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Combining Function Tools with RAG\n",
    "\n",
    "Now for the grand finale—let's combine our calculation functions with document search to create a complete AI assistant. This assistant can both compute astronomical values and search your course materials, choosing the right tool for each question.\n",
    "\n",
    "This combination is powerful: imagine asking \"What's the distance to a star with 0.1 arcsec parallax, and what did we learn about parallax in the course?\" The assistant can calculate the distance AND find relevant course content.\n",
    "\n",
    "### Creating a Search Function for Claude\n",
    "\n",
    "First, let's wrap our RAG search in a function that Claude can call as a tool. This version properly handles complete sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_course_materials(question, max_results=2):\n",
    "    \"\"\"\n",
    "    Search course materials and return relevant content.\n",
    "    This function will be callable by Claude as a tool.\n",
    "    \"\"\"\n",
    "    # Search for relevant chunks\n",
    "    results = search_chunks(question, top_k=max_results)\n",
    "    \n",
    "    # Check if we found anything relevant\n",
    "    if results[0]['similarity'] < 0.2:\n",
    "        return {\n",
    "            \"status\": \"no_relevant_content\",\n",
    "            \"message\": \"No relevant course material found for this question\"\n",
    "        }\n",
    "    \n",
    "    # Format results for Claude\n",
    "    content_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Get section title\n",
    "        lines = result['chunk']['text'].split('\\n')\n",
    "        title = lines[0] if lines else \"No title\"\n",
    "        \n",
    "        # Get content ending at complete sentence\n",
    "        content_text = result['chunk']['text'][:1000]\n",
    "        last_period = content_text.rfind('.')\n",
    "        if last_period > 0:\n",
    "            content_text = content_text[:last_period + 1]\n",
    "        \n",
    "        content_parts.append(f\"Section {i} - {title}:\\n{content_text}\")\n",
    "    \n",
    "    # Return structured results\n",
    "    return {\n",
    "        \"status\": \"found\",\n",
    "        \"best_similarity\": round(results[0]['similarity'], 3),\n",
    "        \"content\": \"\\n\\n\".join(content_parts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Tool Set with Calculations and Search\n",
    "\n",
    "Now let's create our complete tool set that combines astronomical calculations with course material search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete AI Assistant with 3 capabilities:\n",
      "  • parallax_to_distance: Calculate stellar distance from parallax measurement\n",
      "  • stellar_luminosity: Calculate stellar luminosity from radius and temperature\n",
      "  • search_course_materials: Search Lecture 7 notes for relevant course content\n"
     ]
    }
   ],
   "source": [
    "# Complete tools list combining calculations and search\n",
    "complete_tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"parallax_arcsec\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Parallax in arcseconds\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stellar_luminosity\",\n",
    "        \"description\": \"Calculate stellar luminosity from radius and temperature\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"radius_solar\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Radius in solar radii\"\n",
    "                },\n",
    "                \"temperature_k\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Temperature in Kelvin\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"radius_solar\", \"temperature_k\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_course_materials\",\n",
    "        \"description\": \"Search Lecture 7 notes for relevant course content\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"question\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Topic or question to search for\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results (default 2)\",\n",
    "                    \"default\": 2\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Complete AI Assistant with {len(complete_tools)} capabilities:\")\n",
    "for tool in complete_tools:\n",
    "    print(f\"  • {tool['name']}: {tool['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Assistant Function with Natural Language Responses\n",
    "\n",
    "Let's create a complete assistant function that handles the entire workflow, ensuring we always get natural language answers whether Claude uses calculations or searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_assistant(question):\n",
    "    \"\"\"\n",
    "    Complete AI assistant that can calculate and search.\n",
    "    Always returns a natural language answer.\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {question}\")\n",
    "    \n",
    "    # Get Claude's initial response\n",
    "    initial_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=300,\n",
    "        tools=complete_tools,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    \n",
    "    # Check if Claude needs a tool\n",
    "    if initial_response.stop_reason != \"tool_use\":\n",
    "        return initial_response.content[0].text\n",
    "    \n",
    "    # Execute the requested tool\n",
    "    tool_use = initial_response.content[-1]\n",
    "    print(f\"  → Using tool: {tool_use.name}\")\n",
    "    \n",
    "    # Execute the appropriate function\n",
    "    if tool_use.name == \"parallax_to_distance\":\n",
    "        result = parallax_to_distance(tool_use.input['parallax_arcsec'])\n",
    "    elif tool_use.name == \"stellar_luminosity\":\n",
    "        result = stellar_luminosity(\n",
    "            tool_use.input['radius_solar'],\n",
    "            tool_use.input['temperature_k']\n",
    "        )\n",
    "    elif tool_use.name == \"search_course_materials\":\n",
    "        result = search_course_materials(\n",
    "            tool_use.input['question'],\n",
    "            tool_use.input.get('max_results', 2)\n",
    "        )\n",
    "    else:\n",
    "        result = {\"error\": f\"Unknown function: {tool_use.name}\"}\n",
    "    \n",
    "    # Get natural language response\n",
    "    final_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=400,\n",
    "        tools=complete_tools,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": initial_response.content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": str(result)\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return final_response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Complete System\n",
    "\n",
    "Let's test our complete AI assistant with different types of questions—calculations, course content searches, and general questions. Notice how Claude automatically chooses the right tool and provides natural language answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1:\n",
      "============================================================\n",
      "Processing: What's the distance to a star with 0.1 arcsecond parallax?\n",
      "  → Using tool: parallax_to_distance\n",
      "\n",
      "Answer: The distance to a star with a parallax of 0.1 arcseconds is **10 parsecs**.\n",
      "\n",
      "This follows the parallax-distance relationship: distance (in parsecs) = 1 / parallax (in arcseconds). So with a parallax of 0.1 arcseconds, the distance is 1/0.1 = 10 parsecs.\n",
      "\n",
      "For reference, 10 parsecs is equivalent to about 32.6 light-years.\n",
      "============================================================\n",
      "\n",
      "Test 2:\n",
      "============================================================\n",
      "Processing: What did we learn about conversation histories in the API?\n",
      "  → Using tool: search_course_materials\n",
      "\n",
      "Answer: Based on the course materials, here's what we learned about conversation histories in the API:\n",
      "\n",
      "## Key Insight: APIs are Stateless\n",
      "\n",
      "The most important thing we learned is that **unlike the chat interface where Claude seems to \"remember\" your conversation, the API is completely stateless**. This means:\n",
      "\n",
      "- **Each API call is independent** and has no memory of previous calls\n",
      "- Claude does NOT automatically remember what you said earlier\n",
      "- **You must explicitly provide the entire conversation history** with each request to maintain context\n",
      "\n",
      "## Practical Implications\n",
      "\n",
      "The course materials show this with a clear example:\n",
      "\n",
      "1. **First call**: You tell Claude \"My favorite galaxy is M31. Remember this.\"\n",
      "2. **Second call**: If you ask Claude about your favorite galaxy without including the conversation history, Claude will have NO memory of what you said in the first call.\n",
      "\n",
      "## How to Maintain Context\n",
      "\n",
      "To have a proper conversation with Claude through the API, you need to:\n",
      "\n",
      "- Keep track of all previous messages (both user and assistant messages)\n",
      "- Include the complete conversation history in the `messages` parameter of each new API call\n",
      "- Build up the conversation incrementally by appending new messages to your history\n",
      "\n",
      "This is fundamentally different from using Claude's chat interface, where the context is maintained automatically for you. With the API, **you** are responsible for managing the conversation state and providing Claude with the full context it needs to respond appropriately.\n",
      "============================================================\n",
      "\n",
      "Test 3:\n",
      "============================================================\n",
      "Processing: Calculate the luminosity of a star with radius 3 solar radii and temperature 7000K\n",
      "  → Using tool: stellar_luminosity\n",
      "\n",
      "Answer: The star with a radius of 3 solar radii and temperature of 7000K has a luminosity of:\n",
      "\n",
      "- **19.5 solar luminosities** (L☉)\n",
      "- **7.46 × 10²⁷ watts**\n",
      "\n",
      "This means the star is about 19.5 times more luminous than our Sun. The higher luminosity comes from both its larger radius (3 times the Sun's radius) and slightly higher temperature (7000K compared to the Sun's ~5778K).\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test different types of questions\n",
    "test_scenarios = [\n",
    "    \"What's the distance to a star with 0.1 arcsecond parallax?\",\n",
    "    \"What did we learn about conversation histories in the API?\",\n",
    "    \"Calculate the luminosity of a star with radius 3 solar radii and temperature 7000K\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(\"=\" * 60)\n",
    "    answer = complete_assistant(question)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Vector Databases - The Professional Solution\n",
    "\n",
    "### From Our Implementation to Production Systems\n",
    "\n",
    "What we've built today is a fully functional RAG system that works well for single documents or small collections. However, when you're dealing with thousands of documents or millions of chunks in professional research, you need more sophisticated tools called **vector databases**.\n",
    "\n",
    "Vector databases are specialized systems designed to store and search embeddings efficiently. They're like regular databases, but optimized for finding similar vectors quickly, even when you have billions of them.\n",
    "\n",
    "### Three Popular Vector Database Solutions\n",
    "\n",
    "Here are three of the most popular vector database solutions you're likely to encounter:\n",
    "\n",
    "**1. Chroma** - Perfect for getting started\n",
    "Chroma is an open-source, completely free vector database that works seamlessly with Python. It can run entirely in memory for small projects, making it ideal for prototyping and learning. The API feels natural after today's lecture—you'll find the transition straightforward.\n",
    "\n",
    "**2. Pinecone** - The managed cloud solution\n",
    "Pinecone offers a fully managed cloud service where you don't need to maintain any servers. It handles scaling automatically as your data grows, making it more expensive but very reliable and fast. Many production AI applications use Pinecone when they need enterprise-level reliability without the hassle of infrastructure management.\n",
    "\n",
    "**3. FAISS** - Facebook's high-performance library\n",
    "Developed by Facebook AI Research, FAISS is extremely fast, especially with GPU acceleration. It's more of a library than a full database, but when speed is absolutely critical and you need to handle billions of vectors efficiently, FAISS is often the go-to choice.\n",
    "\n",
    "### When to Use Vector Databases\n",
    "\n",
    "Our implementation today works great for single documents or small collections (under 100 documents), prototyping and learning RAG concepts, and understanding how semantic search works under the hood.\n",
    "\n",
    "You should consider upgrading to a vector database when you're working with thousands of documents or research papers, need persistent storage with embeddings saved to disk, have multiple users searching simultaneously, want advanced features like filtering and metadata search, or are building production applications for research teams.\n",
    "\n",
    "### Working Example: ChromaDB\n",
    "\n",
    "Let's see how easy it is to upgrade our system to use ChromaDB. ChromaDB is a vector database that handles storage and search for us, though there are a few important differences from our manual implementation:\n",
    "\n",
    "**Key Differences to Note:**\n",
    "1. **ChromaDB uses its own default embedding model** (not our `all-MiniLM-L6-v2`) unless you explicitly override it\n",
    "2. **ChromaDB returns distances, not similarities** - lower values mean more similar\n",
    "\n",
    "Here's the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install ChromaDB\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing collection to delete\n",
      "Added 12 chunks to ChromaDB\n",
      "\n",
      "Result 1 (distance: 1.279):\n",
      "## Part 1: Understanding APIs\n",
      "\n",
      "### What Is an API, Really?\n",
      "\n",
      "Let's demystify this term that gets thrown around constantly in programming. API stands for Application Programming Interface, but that defi...\n",
      "\n",
      "Result 2 (distance: 1.343):\n",
      "## Part 2: Setting Up Your Connection\n",
      "\n",
      "### Getting Your API Key\n",
      "\n",
      "Before we write any code, you need to obtain an API key from an LLM provider. We'll focus on Anthropic's Claude for this lecture becaus...\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the same embedding model we used\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Read the same Lecture 7 file\n",
    "with open('Lecture7_LLM_API_Basics_20250913.md', 'r') as f:\n",
    "    lecture7_content = f.read()\n",
    "\n",
    "# Use our same chunking function\n",
    "lecture_chunks = chunk_by_sections(lecture7_content)\n",
    "\n",
    "# Create ChromaDB client and clean up any existing collection\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Delete the collection if it already exists\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"lecture7_rag\")\n",
    "    print(\"Deleted existing collection\")\n",
    "except:\n",
    "    print(\"No existing collection to delete\")\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"lecture7_rag\",\n",
    "    metadata={\"description\": \"Lecture 7 content for RAG\"}\n",
    ")\n",
    "\n",
    "# Add all chunks to ChromaDB (it handles embeddings automatically!)\n",
    "for i, chunk in enumerate(lecture_chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk['text']],\n",
    "        ids=[f\"chunk_{i}\"],\n",
    "        metadatas=[{\"chunk_id\": i, \"length\": chunk['length']}]\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(lecture_chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# Now search is incredibly simple\n",
    "results = collection.query(\n",
    "    query_texts=[\"How do I keep API keys secure?\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\nResult {i+1} (distance: {distance:.3f}):\")\n",
    "    print(doc[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Notice how ChromaDB:\n",
    "- Automatically creates embeddings using the same model\n",
    "- Stores everything persistently (survives restarts)\n",
    "- Handles all the vector similarity calculations\n",
    "- Returns results ranked by relevance\n",
    "- Can store metadata alongside each chunk\n",
    "\n",
    "The concepts are identical to what we built—ChromaDB just handles the infrastructure for us. You could now search through hundreds of lecture files without changing the code!\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "What matters isn't the specific vector database you use, but understanding the concepts we've covered today. Documents get chunked into manageable pieces, chunks get converted to embeddings, queries get converted to embeddings, similarity search finds relevant chunks, and retrieved content augments LLM prompts. With this understanding, you can use any vector database—they're all just different implementations of the same core ideas you've mastered today!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Your Computational Research Assistant\n",
    "\n",
    "### What You've Accomplished Today\n",
    "\n",
    "In this lecture, you've built something truly remarkable: an AI assistant that can both perform astronomical calculations and search through your course knowledge base. This isn't just a chatbot—it's a computational research partner that understands your specific materials and can execute your calculations.\n",
    "\n",
    "Let's reflect on the complete journey:\n",
    "\n",
    "**Technical Skills Mastered:**\n",
    "\n",
    "1. **Function Tools**: You learned to convert your Python functions into tools that Claude can call automatically. Any calculation you can code, Claude can now execute. You understand the complete workflow from function definition to natural language response.\n",
    "\n",
    "2. **Document Processing**: You learned about markdown files and how to use Jupytext to convert notebooks. You understand how to prepare documents for computational analysis.\n",
    "\n",
    "3. **Document Chunking**: You understand how to break large documents into manageable, searchable pieces. You've seen both simple section-based chunking and learned about overlapping strategies.\n",
    "\n",
    "4. **Embeddings**: You've demystified how text gets converted to numerical vectors that capture semantic meaning. You understand that similar concepts get similar embeddings, enabling meaning-based search.\n",
    "\n",
    "5. **Cosine Similarity**: You implemented the mathematical foundation of semantic search, understanding how to find related content even when exact words don't match.\n",
    "\n",
    "6. **RAG Implementation**: You built retrieval-augmented generation from scratch, seeing exactly how to ground LLM responses in specific documents rather than general knowledge.\n",
    "\n",
    "7. **Integration**: Most importantly, you combined all these pieces into a unified system where Claude seamlessly switches between calculations and document search, always providing natural language responses.\n",
    "\n",
    "### Why This Matters for Your Research\n",
    "\n",
    "The system you've built today mirrors how professional astronomical research actually works. Modern astronomy involves:\n",
    "- Processing vast amounts of observational data\n",
    "- Searching through thousands of research papers\n",
    "- Running complex calculations repeatedly\n",
    "- Combining literature knowledge with computational analysis\n",
    "\n",
    "You now have the foundational skills to build systems that:\n",
    "- Automate repetitive calculations across large datasets\n",
    "- Search through entire libraries of astronomical literature\n",
    "- Combine multiple sources of information intelligently\n",
    "- Scale from single observations to entire catalogs\n",
    "\n",
    "### The Bigger Picture\n",
    "\n",
    "You've crossed an important threshold today. You're no longer just using AI tools—you're orchestrating them. You understand not just what these systems do, but how they work under the hood. This deep understanding means you can:\n",
    "\n",
    "- Debug when things go wrong (and they will!)\n",
    "- Optimize for your specific research needs\n",
    "- Combine tools in creative ways for novel problems\n",
    "- Build custom solutions that don't exist yet\n",
    "\n",
    "When you encounter a new research challenge, you won't be limited to existing tools. You can build exactly what you need.\n",
    "\n",
    "### Practical Next Steps\n",
    "\n",
    "With these foundations, here's how to apply these skills to your research:\n",
    "\n",
    "1. **Start Small**: Convert one lecture or paper to test your RAG system\n",
    "2. **Build Your Function Library**: Create tools for calculations you use frequently\n",
    "3. **Expand Gradually**: Add more documents and functions as needed\n",
    "4. **Explore Vector Databases**: Try Chroma for larger document collections\n",
    "5. **Share and Collaborate**: Your tools can help other researchers too\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Remember: the goal isn't to replace your astronomical thinking with AI—it's to amplify it. Every function you write encodes your understanding of physics. Every search through course materials reinforces your education. Every integrated system you build makes you a more capable researcher.\n",
    "\n",
    "You now have tools that would have seemed like science fiction just a few years ago. You can have an AI assistant that knows your specific course materials, executes your calculations, and helps you explore astronomical data at unprecedented scale.\n",
    "\n",
    "As you apply these skills to your research projects, remember that you're part of the first generation of astronomers with these capabilities. The universe hasn't changed, but your ability to explore it has expanded dramatically.\n",
    "\n",
    "The questions you can ask, the connections you can discover, and the insights you can derive are limited only by your curiosity and creativity. You have the tools. You understand how they work. Now go forth and use them to advance our understanding of the cosmos.\n",
    "\n",
    "Welcome to the future of computational astronomy—where human insight meets machine capability, and where your curiosity about the universe can be pursued at the speed of thought!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
